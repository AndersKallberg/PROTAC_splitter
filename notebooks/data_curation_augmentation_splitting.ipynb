{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _-----------Data curation--------------------_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from rdkit import DataStructs, Chem\n",
    "from rdkit.Chem import Draw, AllChem\n",
    "\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_curation_augmentation_splitting_functions import (substructure_split_sort, \n",
    "                                                            prepare_data_set, \n",
    "                                                            generate_murcko_in_df, \n",
    "                                                            generate_anonymous_murcko_in_df, \n",
    "                                                            standardize_smiles, \n",
    "                                                            create_group_index_mapping, \n",
    "                                                            collect_unique_substructures,\n",
    "                                                            get_boundary_bondtype,\n",
    "                                                            remove_dummy_atom_from_mol,\n",
    "                                                            remove_dummy_atom_from_smiles,\n",
    "                                                            remove_stereo,\n",
    "                                                            reassemble_protac,\n",
    "                                                            get_murcko,\n",
    "                                                            get_anonymous_murcko,\n",
    "                                                            make_graph_with_pos,\n",
    "                                                            get_test_trainval_smi,\n",
    "                                                            count_unique_SMILES_and_MurckoScaffolds,\n",
    "                                                            validate_test_set,\n",
    "                                                            add_attachments,\n",
    "                                                            validate_no_general_ms_leakage,\n",
    "                                                            generate_protacs_indices,\n",
    "                                                            get_clusters_substructures_attachments_dict,\n",
    "                                                            generate_protac_from_indices_list,\n",
    "                                                            align_mol_2D_ver2,\n",
    "                                                            butina_clustering_substructures_with_fixed_cutoff,\n",
    "                                                            barplot_from_protac_substructureindices_list,\n",
    "                                                            get_bond_idx,\n",
    "                                                            generate_splits)\n",
    "\n",
    "\n",
    "from data_curation_augmentation_splitting_functions import (compute_FP_substructures, \n",
    "                                                            compute_countMorgFP,\n",
    "                                                            compute_RDKitFP)\n",
    "\n",
    "\n",
    "from data_curation_augmentation_splitting_functions import (save_as_svg,\n",
    "                                                            align_molecules_2D,\n",
    "                                                            align_molecules_by_coordinates,\n",
    "                                                            draw_molecule_with_highlighted_bonds,\n",
    "                                                            tailored_framework_example,\n",
    "                                                            transform_molecule,\n",
    "                                                            draw_molecule_to_svg,\n",
    "                                                            combine_svgs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('/../data/train_dataset.csv')\n",
    "validation_dataset = pd.read_csv('/../data/validation_dataset.csv')\n",
    "\n",
    "trainset_protac = train_dataset['smiles']\n",
    "trainset_substructures = train_dataset['substructures']\n",
    "\n",
    "validationset_protac = validation_dataset['smiles']\n",
    "validationset_substructures = validation_dataset['substructures']\n",
    "\n",
    "\n",
    "#cwd = os.getcwd()\n",
    "#data_path = cwd + '/../data/curated_dataset.csv'\n",
    "\n",
    "#your_huggingface_token = \"token\" #this source may be set to private. In this case, load the \"curated_dataset.csv\" instead.\n",
    "\n",
    "#if os.path.isfile(data_path):\n",
    "#    with open(data_path, \"rb\") as file:\n",
    "#        dataset_aibe = pickle.load(file)\n",
    "#else:\n",
    "#    name_dataset_aibe = \"80-20-split\"\n",
    "#    dataset_aibe = load_dataset(\"ailab-bio/PROTAC-Substructures\", name_dataset_aibe, token=your_huggingface_token) #download use_auth_token=True\n",
    "\n",
    "#train_dataset = dataset_aibe['train']\n",
    "#validation_dataset = dataset_aibe['validation']\n",
    "\n",
    "#trainset_protac = train_dataset['text']\n",
    "#trainset_substructures = train_dataset['labels']\n",
    "\n",
    "#validationset_protac = validation_dataset['text']\n",
    "#validationset_substructures = validation_dataset['labels']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data & remove ambigous substructure matches (matching can be improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge training and validation data, and create dataframe of public PROTACs, POI, Linker, E3.\n",
    "\n",
    "protac_smi_list = trainset_protac + validationset_protac\n",
    "substructures_list = trainset_substructures + validationset_substructures\n",
    "\n",
    "poi_smi_r_list = []\n",
    "linker_smi_r_list = []\n",
    "e3_smi_r_list = []\n",
    "for substructures in substructures_list:\n",
    "    poi_smi_r, linker_smi_r, e3_smi_r = substructure_split_sort(substructures)\n",
    "    poi_smi_r_list.append(poi_smi_r)\n",
    "    linker_smi_r_list.append(linker_smi_r)\n",
    "    e3_smi_r_list.append(e3_smi_r)\n",
    "\n",
    "pub_smi_df = pd.DataFrame({\n",
    "    'Smiles': protac_smi_list,\n",
    "    'POI': poi_smi_r_list,\n",
    "    'Linker': linker_smi_r_list,\n",
    "    'E3': e3_smi_r_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes protacs with ambigous substructure matches. Split dataframe by PROTACs and substructures\n",
    "pub_protac_smi_prep_df, pub_substructure_unsplitted_smi_prep_df = prepare_data_set(pub_smi_df, 'Smiles', 'POI', 'Linker', 'E3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dataframes again = Create dataframe of public PROTACs, POI, Linker, E3, where there are no ambigous substructure matches\n",
    "\n",
    "protac_smi_prep_list = []\n",
    "poi_smi_r_prep_list = []\n",
    "linker_smi_r_prep_list = []\n",
    "e3_smi_r_prep_list = []\n",
    "for idx, row in pub_substructure_unsplitted_smi_prep_df.iterrows():\n",
    "    substructures = row[\"substructures\"]\n",
    "    protac_smi_prep = pub_protac_smi_prep_df.loc[idx,\"Smiles\"]\n",
    "    protac_smi_prep_list.append(protac_smi_prep)\n",
    "    poi_smi_prep_r, linker_smi_prep_r, e3_smi_prep_r = substructure_split_sort(substructures)\n",
    "    poi_smi_r_prep_list.append(poi_smi_prep_r)\n",
    "    linker_smi_r_prep_list.append(linker_smi_prep_r)\n",
    "    e3_smi_r_prep_list.append(e3_smi_prep_r)\n",
    "\n",
    "pub_smi_prep_df = pd.DataFrame({\n",
    "    'Smiles': protac_smi_prep_list,\n",
    "    'POI': poi_smi_r_prep_list,\n",
    "    'Linker': linker_smi_r_prep_list,\n",
    "    'E3': e3_smi_r_prep_list\n",
    "})\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Framework SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get murcko scaffold & framework\n",
    "for col in [\"Smiles\", \"POI\", \"Linker\", \"E3\"]:\n",
    "    pub_smi_prep_df = generate_murcko_in_df(pub_smi_prep_df, col)\n",
    "    pub_smi_prep_df = generate_anonymous_murcko_in_df(pub_smi_prep_df, col)\n",
    "\n",
    "#Ensure SMILES are standardized\n",
    "all_structures = [\"Smiles\", \"POI\", \"Linker\", \"E3\", \"Smiles_MS\", \"Smiles_AnonMS\", \"POI_MS\", \"POI_AnonMS\", \"Linker_MS\", \"Linker_AnonMS\", \"E3_MS\", \"E3_AnonMS\"]\n",
    "for col in all_structures:\n",
    "    pub_smi_prep_df[col] =  pub_smi_prep_df[col].apply(standardize_smiles)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique standardized FRAMEWORK SMILES\n",
    "unique_abstract_poi_smi_list = pub_smi_prep_df['POI_AnonMS'].dropna().unique().tolist()\n",
    "unique_abstract_linker_smi_list = pub_smi_prep_df['Linker_AnonMS'].dropna().unique().tolist()\n",
    "unique_abstract_e3_smi_list = pub_smi_prep_df['E3_AnonMS'].dropna().unique().tolist()\n",
    "\n",
    "# Create groups for POI, Linker, and E3 for those which have the same abstract SMILES\n",
    "poi_AnonMS_to_group = create_group_index_mapping(unique_abstract_poi_smi_list)\n",
    "linker_AnonMS_to_group = create_group_index_mapping(unique_abstract_linker_smi_list)         #Dictionary mapping a framework to it respective ID\n",
    "e3_AnonMS_to_group = create_group_index_mapping(unique_abstract_e3_smi_list)\n",
    "\n",
    "# Assign group indices in dataframe\n",
    "pub_smi_prep_df['POI_Group'] = pub_smi_prep_df['POI_AnonMS'].map(poi_AnonMS_to_group)\n",
    "pub_smi_prep_df['Linker_Group'] = pub_smi_prep_df['Linker_AnonMS'].map(linker_AnonMS_to_group)\n",
    "pub_smi_prep_df['E3_Group'] = pub_smi_prep_df['E3_AnonMS'].map(e3_AnonMS_to_group)\n",
    "\n",
    "# Make dictionaries containing groups for substructures for POIs, linkers, and E3s which have the same abstract SMILES\n",
    "poi_AnonMSgroup_to_smi = collect_unique_substructures(pub_smi_prep_df, 'POI_Group', 'POI')\n",
    "linker_AnonMSgroup_to_smi = collect_unique_substructures(pub_smi_prep_df, 'Linker_Group', 'Linker')\n",
    "e3_AnonMSgroup_to_smi = collect_unique_substructures(pub_smi_prep_df, 'E3_Group', 'E3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign an ID for each substructure SMILES\n",
    "POI_ids, _ = pd.factorize(pub_smi_prep_df['POI'])\n",
    "pub_smi_prep_df[\"POI_ID\"] = POI_ids\n",
    "Linker_ids, _ = pd.factorize(pub_smi_prep_df['Linker'])\n",
    "pub_smi_prep_df[\"Linker_ID\"] = Linker_ids\n",
    "E3_ids, _ = pd.factorize(pub_smi_prep_df['E3'])\n",
    "pub_smi_prep_df[\"E3_ID\"] = E3_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each unique smile, get the id, go through all rows in the dataframe with the same id, validate that the unique smile matches these smiles in the dataframe for the same ids\n",
    "#If you get no errors, everything is fine\n",
    "\n",
    "unique_poi = pub_smi_prep_df['POI'].dropna().unique().tolist()\n",
    "unique_linker = pub_smi_prep_df['Linker'].dropna().unique().tolist()\n",
    "unique_e3 = pub_smi_prep_df['E3'].dropna().unique().tolist()\n",
    "\n",
    "for s, unique_s in zip([\"POI\", \"Linker\", \"E3\"], [unique_poi, unique_linker, unique_e3]):\n",
    "    for unique_id, smi_unique in enumerate(unique_s):\n",
    "        condition = pub_smi_prep_df[f\"{s}_ID\"] == unique_id\n",
    "        smi = pub_smi_prep_df.loc[condition,f\"{s}\"].dropna().unique().tolist() \n",
    "        if len(smi)>1:\n",
    "            raise ValueError(\"not unique\")\n",
    "        if smi_unique != smi[0]: \n",
    "            raise ValueError(\"not matching\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move E3 in POI list to E3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute fingerprint for all substructures\n",
    "for col in [\"POI\", \"Linker\" , \"E3\", \"POI_MS\", \"Linker_MS\", \"E3_MS\"]:\n",
    "    fps = compute_FP_substructures(pub_smi_prep_df, [col], fp_function=compute_countMorgFP, return_unique=False, convert_to_numpyarray = True)\n",
    "    pub_smi_prep_df[f\"{col}_MorgFP\"] = fps[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_unique_fps = compute_FP_substructures(pub_smi_prep_df, [\"POI\"], fp_function=compute_countMorgFP, return_unique=True, convert_to_numpyarray = False)\n",
    "poi_unique_fps = poi_unique_fps[0]\n",
    "e3_unique_fps = compute_FP_substructures(pub_smi_prep_df, [\"E3\"], fp_function=compute_countMorgFP, return_unique=True, convert_to_numpyarray = False)\n",
    "e3_unique_fps = e3_unique_fps[0]\n",
    "\n",
    "similarity_matrix_poi_e3 = np.zeros((len(poi_unique_fps), len(e3_unique_fps)))\n",
    "for i in range(len(poi_unique_fps)):\n",
    "    for j in range(0, len(e3_unique_fps)):\n",
    "        similarity = DataStructs.TanimotoSimilarity(poi_unique_fps[i], e3_unique_fps[j])\n",
    "        similarity_matrix_poi_e3[i][j] = similarity\n",
    "\n",
    "row_max_values = np.max(similarity_matrix_poi_e3, axis=1) # Maximum values in each row\n",
    "row_max_column_indices = np.argmax(similarity_matrix_poi_e3, axis=1) # Column indices for max values in each row\n",
    "\n",
    "\n",
    "\n",
    "poi_fps = compute_FP_substructures(pub_smi_prep_df, [\"POI\"], fp_function=compute_countMorgFP, return_unique=False, convert_to_numpyarray = False)\n",
    "poi_fps = poi_fps[0]\n",
    "e3_fps = compute_FP_substructures(pub_smi_prep_df, [\"E3\"], fp_function=compute_countMorgFP, return_unique=False, convert_to_numpyarray = False)\n",
    "e3_fps = e3_fps[0]\n",
    "\n",
    "similarity_matrix_poi_poi = np.zeros((len(poi_fps), len(poi_fps)))\n",
    "for i in range(len(poi_fps)):\n",
    "    for j in range(0, len(poi_fps)):\n",
    "        #print(poi_unique_fps[i])\n",
    "        similarity = DataStructs.TanimotoSimilarity(poi_fps[i], poi_fps[j])\n",
    "        similarity_matrix_poi_poi[i][j] = similarity\n",
    "\n",
    "similarity_matrix_e3_e3 = np.zeros((len(e3_fps), len(e3_fps)))\n",
    "for i in range(len(e3_fps)):\n",
    "    for j in range(0, len(e3_fps)):\n",
    "        #print(poi_unique_fps[i])\n",
    "        similarity = DataStructs.TanimotoSimilarity(e3_fps[i], e3_fps[j])\n",
    "        similarity_matrix_e3_e3[i][j] = similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot suspected E3 in POI list, which matches well to an E3 in the E3 list.                           OBS! This does not do anything, its just for visualizing!\n",
    "\n",
    "\n",
    "\n",
    "# Sort rows by their max values and get the top 10\n",
    "sorted_row_indices_top10 = np.argsort(row_max_values)[::-1][:10] # Indices of rows sorted by max value\n",
    "\n",
    "show_structures = True\n",
    "# Display top rows, their max values, and the column indices for these max values\n",
    "if show_structures:\n",
    "    #print(\"\\nTop Rows:\")\n",
    "    for idx in sorted_row_indices_top10:\n",
    "        print(f\"E3s in POI list:   POI Index: {idx}, Max Value: {row_max_values[idx]}, E3 Index: {row_max_column_indices[idx]}\")\n",
    "        \n",
    "        \n",
    "        print(f'The suspected E3 occurs {sum(pub_smi_prep_df[\"POI_ID\"]==idx)} times among the POI in all PROTACs, whereas the E3 it matched best against occurs {sum(pub_smi_prep_df[\"E3_ID\"]==row_max_column_indices[idx])} times among the E3 in all PROTACs')\n",
    "        poi_group_of_poi_id = pub_smi_prep_df[pub_smi_prep_df[\"POI_ID\"]==idx]['POI_Group'].unique().tolist()[0]\n",
    "        poi_framework_smi = pub_smi_prep_df[pub_smi_prep_df[\"POI_Group\"]==poi_group_of_poi_id][\"POI_AnonMS\"].tolist()[0]\n",
    "        e3_group_of_e3_id = pub_smi_prep_df[pub_smi_prep_df[\"E3_ID\"]==row_max_column_indices[idx]]['E3_Group'].unique().tolist()[0]\n",
    "        e3_framework_smi = pub_smi_prep_df[pub_smi_prep_df[\"E3_Group\"]==e3_group_of_e3_id][\"E3_AnonMS\"].tolist()[0]\n",
    "        if  row_max_values[idx]> 0.4 and e3_framework_smi == poi_framework_smi:\n",
    "            print(\"E3 and POI (suspected E3) have the same graph framework:\")\n",
    "            print(\"POI:\")\n",
    "            poi_smi = unique_poi[idx]\n",
    "            display(Chem.MolFromSmiles(poi_smi))\n",
    "            print(poi_smi)\n",
    "            print(\"E3:\")\n",
    "            e3_smi = unique_e3[row_max_column_indices[idx]]\n",
    "            display(Chem.MolFromSmiles(e3_smi))\n",
    "            print(e3_smi)\n",
    "\n",
    "\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moves E3 from POI list to E3 list, if above cutoff and same framework.       If this is run multiple times, it will fail (as is). Run only once.\n",
    "\n",
    "save_fig = False\n",
    "\n",
    "e3_among_poi = []\n",
    "poi_among_e3 = []\n",
    "sorted_row_indices = np.argsort(row_max_values)[::-1]\n",
    "pairing_cutoff = 0.4 # low as to find all matches between the lists of E3 and Warheads\n",
    "move_id = 0\n",
    "for poi_idx in sorted_row_indices: #Go thorugh all smi in the POI list\n",
    "    tanimoto_similatity = row_max_values[poi_idx]\n",
    "    if  tanimoto_similatity > pairing_cutoff:  #If any smi in POI list matches to any E3 about the cutoff\n",
    "        e3_idx = row_max_column_indices[poi_idx]\n",
    "\n",
    "        poi_group_of_poi_id = pub_smi_prep_df[pub_smi_prep_df[\"POI_ID\"]==poi_idx]['POI_Group'].tolist()[0]\n",
    "        poi_framework_smi = pub_smi_prep_df[pub_smi_prep_df[\"POI_Group\"]==poi_group_of_poi_id][\"POI_AnonMS\"].tolist()[0]\n",
    "\n",
    "        e3_group_of_e3_id = pub_smi_prep_df[pub_smi_prep_df[\"E3_ID\"]==e3_idx]['E3_Group'].tolist()[0]\n",
    "        e3_framework_smi = pub_smi_prep_df[pub_smi_prep_df[\"E3_Group\"]==e3_group_of_e3_id][\"E3_AnonMS\"].tolist()[0]\n",
    "\n",
    "        poi_smi = unique_poi[poi_idx]\n",
    "        e3_smi = unique_e3[e3_idx]\n",
    "        \n",
    "        poi_mol = Chem.MolFromSmiles(poi_smi)\n",
    "        e3_mol = Chem.MolFromSmiles(e3_smi)\n",
    "\n",
    "        framework_str = \"e3_(maybe)_among_poi_different_frameworks\"\n",
    "        if e3_framework_smi == poi_framework_smi:\n",
    "            framework_str = \"e3_among_poi_same_framework\"\n",
    "\n",
    "        print(f\"Pair of Warhead and E3 matches to {tanimoto_similatity} (above {pairing_cutoff}) {framework_str}\")\n",
    "        print(\"POI\")\n",
    "        display(poi_mol)\n",
    "        print(\"E3\")\n",
    "        display(e3_mol)\n",
    "\n",
    "\n",
    "        if e3_framework_smi == poi_framework_smi: #matches_of_e3_to_other_e3 > matches_of_poi_to_other_poi:             #the pair are both most likely E3's as the E3 in the pair matches more often to other E3s than the 'Warhead' matches more to other warheads\n",
    "            e3_among_poi.append(poi_idx)\n",
    "        svg_name = f'{framework_str}_{move_id}_sim{round(tanimoto_similatity,4)}'\n",
    "        svg_path = f\"{os.getcwd()}/fig_method/{svg_name}.svg\"\n",
    "        e3_mol = align_molecules_2D(poi_mol, e3_mol)\n",
    "        substructures_img = Draw.MolsToGridImage([poi_mol, e3_mol], subImgSize=(500, 500), useSVG=True)\n",
    "        move_id += 1\n",
    "\n",
    "        if save_fig:\n",
    "            save_as_svg(substructures_img, svg_path, num_mols=2)\n",
    "            display(SVG(filename=svg_path))\n",
    "        \n",
    "        \n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e3_among_poi_smi = [unique_poi[idx] for idx in e3_among_poi]\n",
    "unique_poi = list(set(unique_poi)-set(e3_among_poi_smi))\n",
    "\n",
    "poi_among_e3_smi = [unique_e3[idx] for idx in poi_among_e3]\n",
    "unique_e3 = list(set(unique_e3)-set(poi_among_e3_smi))\n",
    "\n",
    "#Change attatchment point from [*:1] to [*:2] before transferring to E3-list\n",
    "e3_among_poi_smi = [smi.replace(\"[*:1]\", \"[*:2]\") for smi in e3_among_poi_smi]\n",
    "poi_among_e3_smi = [smi.replace(\"[*:2]\", \"[*:1]\") for smi in poi_among_e3_smi]\n",
    "\n",
    "unique_e3 = list(set(unique_e3 + e3_among_poi_smi)) #Add E3s from POI list. Make sure there are no duplicates\n",
    "unique_poi = list(set(unique_poi + poi_among_e3_smi)) #Add E3s from POI list. Make sure there are no duplicates\n",
    "\n",
    "#In the future: only delete from unique_poi, or modify the dataframe (maybe replace these \"POI\" with a string saying 'E3'?) - Do not forget. This means to not extract POI from the dataframe directly\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (ETC) Analyze distribution of boundary bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_substructures_with_attachments = {'POI': unique_poi, 'LINKER': unique_linker, 'E3': unique_e3}\n",
    "bondtype_count_substructures =  {}\n",
    "for substruct_str in unique_substructures_with_attachments.keys():\n",
    "    bondtype_count = {'SINGLE': 0, 'DOUBLE': 0, 'TRIPLE': 0}\n",
    "    substructure_plural_smi_with_attachment = unique_substructures_with_attachments[substruct_str]\n",
    "    for substructure_smi_with_attachment in substructure_plural_smi_with_attachment:\n",
    "        substruct_mol_with_attachment = Chem.MolFromSmiles(substructure_smi_with_attachment)\n",
    "        bondtype_count = get_boundary_bondtype(mol=substruct_mol_with_attachment, bondtype_count=bondtype_count)\n",
    "    bondtype_count_substructures[substruct_str] = bondtype_count\n",
    "\n",
    "print(\"Count of each bond type for all unique substructures, grouped by substructure type:\")\n",
    "print(bondtype_count_substructures)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get substructures without attachmentpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_substructures_with_attachments = {'POI': unique_poi, 'LINKER': unique_linker, 'E3': unique_e3}\n",
    "unique_substructures_without_attachments = {'POI': [], 'LINKER': [], 'E3': []}\n",
    "\n",
    "unique_substructures_without_attachments_dict_to_with_attachments = {'POI': {}, 'LINKER': {}, 'E3': {}}\n",
    "\n",
    "\n",
    "\n",
    "for substruct in unique_substructures_without_attachments.keys():\n",
    "\n",
    "\n",
    "\n",
    "    substructures_smi_with_attachment = unique_substructures_with_attachments[substruct]\n",
    "    for substructure_smi_with_attachment in substructures_smi_with_attachment:\n",
    "        \n",
    "        substruct_mol_with_attachment = Chem.MolFromSmiles(substructure_smi_with_attachment)\n",
    "        substruct_mol_without_attachment = remove_dummy_atom_from_mol(mol=substruct_mol_with_attachment, output = \"smiles\")\n",
    "        unique_substructures_without_attachments[substruct].append(substruct_mol_without_attachment)\n",
    "\n",
    "        if substruct_mol_without_attachment not in unique_substructures_without_attachments_dict_to_with_attachments[substruct]:\n",
    "            unique_substructures_without_attachments_dict_to_with_attachments[substruct][substruct_mol_without_attachment] = []\n",
    "        unique_substructures_without_attachments_dict_to_with_attachments[substruct][substruct_mol_without_attachment].append(substructure_smi_with_attachment)\n",
    "\n",
    "unique_poi_without_attachment = list(set(unique_substructures_without_attachments['POI']))\n",
    "unique_linker_without_attachment = list(set(unique_substructures_without_attachments['LINKER']))\n",
    "unique_e3_without_attachment = list(set(unique_substructures_without_attachments['E3']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _-----------Data split------------------------_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasplit via Tanimoto similarity (HDBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_without_attachment_test_trainval_smi_dict = get_test_trainval_smi(smi_list = unique_poi_without_attachment, \n",
    "                                                                        fp_function = compute_countMorgFP,\n",
    "                                                                        max_allowed_tanimoto_similarity = 0.45, \n",
    "                                                                        test_set_minfraction = 0.15, \n",
    "                                                                        test_set_maxfraction = 0.30, \n",
    "                                                                        binwidth_plural = [5, 1],\n",
    "                                                                        substructure_type='poi')\n",
    "\n",
    "poi_test_smi_set_without_attachment_splits = poi_without_attachment_test_trainval_smi_dict[\"test\"]\n",
    "poi_trainval_smi_set_without_attachment = poi_without_attachment_test_trainval_smi_dict[\"trainval\"]\n",
    "\n",
    "for split_idx, test_split in poi_test_smi_set_without_attachment_splits.items():\n",
    "    print(f' Test counts of POI, without attachment, split idx {split_idx}: {count_unique_SMILES_and_MurckoScaffolds(test_split)}')\n",
    "print(f' Trainval counts of POI: {count_unique_SMILES_and_MurckoScaffolds(poi_trainval_smi_set_without_attachment)}')\n",
    "\n",
    "print(\"\\n ----------------------------------------------------------------- \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#remove \"linker\" with no size, which directly joins POI and E3\n",
    "while \"[*:1][*:2]\" in unique_linker:\n",
    "    unique_linker.remove(\"[*:1][*:2]\")\n",
    "while \"[*:2][*:1]\" in unique_linker:\n",
    "    unique_linker.remove(\"[*:2][*:1]\")\n",
    "\n",
    "while \"[H][H]\" in unique_linker_without_attachment:\n",
    "    unique_linker_without_attachment.remove(\"[H][H]\")\n",
    "\n",
    "linker_without_attachment_test_trainval_smi_dict = get_test_trainval_smi(smi_list = unique_linker_without_attachment, \n",
    "                                                                        fp_function = compute_RDKitFP,\n",
    "                                                                        max_allowed_tanimoto_similarity = 0.45, \n",
    "                                                                        test_set_minfraction = 0.15, \n",
    "                                                                        test_set_maxfraction = 0.30, \n",
    "                                                                        binwidth_plural = [5, 1],\n",
    "                                                                        substructure_type='linker')\n",
    "\n",
    "linker_test_smi_set_without_attachment_splits = linker_without_attachment_test_trainval_smi_dict[\"test\"]\n",
    "linker_trainval_smi_set_without_attachment = linker_without_attachment_test_trainval_smi_dict[\"trainval\"]\n",
    "\n",
    "for split_idx, test_split in linker_test_smi_set_without_attachment_splits.items():\n",
    "    print(f' Test counts of linker, without attachment, split idx {split_idx}: {count_unique_SMILES_and_MurckoScaffolds(test_split)}')\n",
    "print(f' Trainval counts of linker: {count_unique_SMILES_and_MurckoScaffolds(linker_trainval_smi_set_without_attachment)}')\n",
    "\n",
    "print(\"\\n ----------------------------------------------------------------- \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e3_without_attachment_test_trainval_smi_dict = get_test_trainval_smi(smi_list = unique_e3_without_attachment, \n",
    "                                                                        fp_function = compute_countMorgFP,\n",
    "                                                                        max_allowed_tanimoto_similarity = 0.5, \n",
    "                                                                        test_set_minfraction = 0.2, \n",
    "                                                                        test_set_maxfraction = 0.30, \n",
    "                                                                        binwidth_plural = [5, 1],\n",
    "                                                                        substructure_type='e3')\n",
    "\n",
    "e3_test_smi_set_without_attachment_splits = e3_without_attachment_test_trainval_smi_dict[\"test\"]\n",
    "e3_trainval_smi_set_without_attachment = e3_without_attachment_test_trainval_smi_dict[\"trainval\"]\n",
    "\n",
    "for split_idx, test_split in e3_test_smi_set_without_attachment_splits.items():\n",
    "    print(f' Test counts of e3, without attachment, split idx {split_idx}: {count_unique_SMILES_and_MurckoScaffolds(test_split)}')\n",
    "print(f' Trainval counts of e3: {count_unique_SMILES_and_MurckoScaffolds(e3_trainval_smi_set_without_attachment)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate generate recombined PROTACs and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_cutoff = 0.33\n",
    "split_dict = generate_splits(poi_test_smi_set_without_attachment_splits, linker_test_smi_set_without_attachment_splits, e3_test_smi_set_without_attachment_splits,\n",
    "                    poi_trainval_smi_set_without_attachment, linker_trainval_smi_set_without_attachment, e3_trainval_smi_set_without_attachment,\n",
    "                    unique_substructures_without_attachments_dict_to_with_attachments, unique_substructures_with_attachments, fixed_cutoff)\n",
    "\n",
    "butina_cluster_cutoff = f'ButinaClusterCutoff_{fixed_cutoff}'\n",
    "for split_idx, datasets in split_dict.items():\n",
    "    if \"Train\" in datasets:\n",
    "        train_df = datasets[\"Train\"]\n",
    "        train_df.to_csv(f'../data/augmented/train_{len(train_df)}_{butina_cluster_cutoff}.csv', index=False)\n",
    "    \n",
    "    if \"Validation\" in datasets:\n",
    "        val_df = datasets[\"Validation\"]\n",
    "        val_df.to_csv(f'../data/augmented/val_{len(val_df)}_{butina_cluster_cutoff}.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_poi_df = datasets[\"Test POI\"]\n",
    "    test_linker_df = datasets[\"Test Linker\"]\n",
    "    test_e3_df = datasets[\"Test E3\"]\n",
    "    test_poilinker_df = datasets[\"Test POILINKER\"]\n",
    "    test_poie3_df = datasets[\"Test POIE3\"]\n",
    "    test_e3linker_df = datasets[\"Test E3Linker\"]\n",
    "    test_protac_df = datasets[\"Test PROTAC\"]\n",
    "\n",
    "\n",
    "    test_poi_df.to_csv(f'../data/augmented/test_poi_split{split_idx}_{butina_cluster_cutoff}.csv', index=False)\n",
    "    test_linker_df.to_csv(f'../data/augmented/test_linker_split{split_idx}_{butina_cluster_cutoff}.csv', index=False)\n",
    "    test_e3_df.to_csv(f'../data/augmented/test_e3_split{split_idx}_{butina_cluster_cutoff}.csv', index=False)\n",
    "    test_poilinker_df.to_csv(f'../data/augmented/test_poilinker_split{split_idx}_{butina_cluster_cutoff}.csv', index=False)\n",
    "    test_poie3_df.to_csv(f'../data/augmented/test_poie3_split{split_idx}_{butina_cluster_cutoff}.csv', index=False)\n",
    "    test_e3linker_df.to_csv(f'../data/augmented/test_e3linker_split{split_idx}_{butina_cluster_cutoff}.csv', index=False)\n",
    "    test_protac_df.to_csv(f'../data/augmented/test_protac_split{split_idx}_{butina_cluster_cutoff}.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save various train & val dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainval_train_to_val_fraction = 1/trainval_fraction - 1\n",
    "\n",
    "generate_larger_trainsizes = True\n",
    "\n",
    "if generate_larger_trainsizes:\n",
    "    trainval_smi_set_without_attachment = [poi_trainval_smi_set_without_attachment, linker_trainval_smi_set_without_attachment, e3_trainval_smi_set_without_attachment]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    trainval_clusters_for_substructures = butina_clustering_substructures_with_fixed_cutoff(\n",
    "                                                                smi_sets_without_attachment = trainval_smi_set_without_attachment, \n",
    "                                                                cutoff = fixed_cutoff,\n",
    "                                                                plot_top_clusters = [], \n",
    "                                                                plot = True,\n",
    "                                                                yscale='linear',\n",
    "                                                                test_or_trainval = \"trainval\")\n",
    "\n",
    "    trainval_clusters_substructures_attachments_dict = get_clusters_substructures_attachments_dict(trainval_clusters_for_substructures,\n",
    "                                                                                                    trainval_smi_set_without_attachment,\n",
    "                                                                                                    unique_substructures_without_attachments_dict_to_with_attachments)\n",
    "\n",
    "    trainval_fraction = 0.2 \n",
    "    for num_train_protacs_factor in [3, 10]:\n",
    "\n",
    "        num_trainval_protacs = num_train_protacs_factor/(1-trainval_fraction)\n",
    "        num_protacs_factor = num_trainval_protacs*(1-trainval_fraction) + num_trainval_protacs*trainval_fraction # the number new augmented unique protacs equal to num_protacs_factor multiplied by the number of substructures which there are most\n",
    "        trainval_augmented_protac_substructureindices_list, trainval_clusters_substructures_attachments_dict_out = generate_protacs_indices(clusters_dict=trainval_clusters_substructures_attachments_dict, \n",
    "                                                                                                                    num_protacs_factor=num_protacs_factor,\n",
    "                                                                                                                    force_use_all_attachment_points=True)\n",
    "\n",
    "        bond_type = 'rand_uniform'\n",
    "        trainval_smiles_with_attachments = generate_protac_from_indices_list(trainval_clusters_substructures_attachments_dict_out,\n",
    "                                                                            trainval_augmented_protac_substructureindices_list,\n",
    "                                                                            bond_type = bond_type)\n",
    "        \n",
    "        trainval_df = pd.DataFrame(trainval_smiles_with_attachments) #spit df into training and validation\n",
    "\n",
    "        train_df = trainval_df.sample(frac=1-trainval_fraction)\n",
    "        val_df = trainval_df.drop(train_df.index)\n",
    "\n",
    "        \n",
    "        butina_cluster_cutoff = f'ButinaClusterCutoff_{fixed_cutoff}'\n",
    "\n",
    "        train_df.to_csv(f'../data/augmented/train_{len(train_df)}_{butina_cluster_cutoff}.csv', index=False)\n",
    "        val_df.to_csv(f'../data/augmented/val_{len(val_df)}_{butina_cluster_cutoff}.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROTACs recombined from ALL substructures\n",
    "\n",
    "For training on all available data, to get a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainval_train_to_val_fraction = 1/trainval_fraction - 1\n",
    "\n",
    "generate_larger_trainsizes = True\n",
    "\n",
    "if generate_larger_trainsizes:\n",
    "    trainval_smi_set_without_attachment = [list(set(substructures)) for substructures in list(unique_substructures_without_attachments.values())] #\n",
    "    \n",
    "    while \"[H][H]\" in trainval_smi_set_without_attachment[1]:\n",
    "        trainval_smi_set_without_attachment[1].remove(\"[H][H]\")\n",
    "    \n",
    "\n",
    "    \n",
    "    trainval_clusters_for_substructures = butina_clustering_substructures_with_fixed_cutoff(\n",
    "                                                                smi_sets_without_attachment = trainval_smi_set_without_attachment, \n",
    "                                                                cutoff = fixed_cutoff,\n",
    "                                                                plot_top_clusters = [], \n",
    "                                                                plot = True,\n",
    "                                                                yscale='linear',\n",
    "                                                                test_or_trainval = \"trainval\")\n",
    "\n",
    "    trainval_clusters_substructures_attachments_dict = get_clusters_substructures_attachments_dict(trainval_clusters_for_substructures,\n",
    "                                                                                                    trainval_smi_set_without_attachment,\n",
    "                                                                                                    unique_substructures_without_attachments_dict_to_with_attachments)\n",
    "\n",
    "    trainval_fraction = 0.2 \n",
    "    for num_train_protacs_factor in [10]:\n",
    "\n",
    "        num_trainval_protacs = num_train_protacs_factor/(1-trainval_fraction)\n",
    "        num_protacs_factor = num_trainval_protacs*(1-trainval_fraction) + num_trainval_protacs*trainval_fraction # the number new augmented unique protacs equal to num_protacs_factor multiplied by the number of substructures which there are most\n",
    "        trainval_augmented_protac_substructureindices_list, trainval_clusters_substructures_attachments_dict_out = generate_protacs_indices(clusters_dict=trainval_clusters_substructures_attachments_dict, \n",
    "                                                                                                                    num_protacs_factor=num_protacs_factor,\n",
    "                                                                                                                    force_use_all_attachment_points=True)\n",
    "\n",
    "        bond_type = 'rand_uniform'\n",
    "        trainval_smiles_with_attachments = generate_protac_from_indices_list(trainval_clusters_substructures_attachments_dict_out,\n",
    "                                                                            trainval_augmented_protac_substructureindices_list,\n",
    "                                                                            bond_type = bond_type)\n",
    "        \n",
    "        trainval_df = pd.DataFrame(trainval_smiles_with_attachments) #spit df into training and validation\n",
    "\n",
    "        train_df = trainval_df.sample(frac=1-trainval_fraction)\n",
    "        val_df = trainval_df.drop(train_df.index)\n",
    "\n",
    "        \n",
    "        butina_cluster_cutoff = f'ButinaClusterCutoff_{fixed_cutoff}'\n",
    "\n",
    "        train_df.to_csv(f'../data/augmented/train_all_{len(train_df)}_{butina_cluster_cutoff}.csv', index=False)\n",
    "        val_df.to_csv(f'../data/augmented/val_all_{len(val_df)}_{butina_cluster_cutoff}.csv', index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-protac-toolkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
